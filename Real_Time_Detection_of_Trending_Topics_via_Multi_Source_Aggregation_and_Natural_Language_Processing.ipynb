{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gPrgePHh_Ku",
        "outputId": "53a750b3-d881-4f12-9592-0bdc958b6273"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.11/dist-packages (6.0.11)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: praw in /usr/local/lib/python3.11/dist-packages (7.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.13)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.11/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.11/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install requests feedparser matplotlib wordcloud nltk gradio praw scikit-learn spacy\n",
        "!python -m nltk.downloader stopwords\n",
        "!python -m nltk.downloader vader_lexicon\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import feedparser\n",
        "import io\n",
        "import base64\n",
        "from datetime import datetime, timedelta\n",
        "from collections import Counter, defaultdict\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "import gradio as gr\n",
        "import praw\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "GNEWS_API_KEY = \"46a3e5dbc2ac726efc117a977791d605\"\n",
        "NEWSAPI_KEY = \"6e6471621d62400d8a81a61799b715e9\"\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"RK5_wfgydhs-NaIJ1y6pqg\",\n",
        "    client_secret=\"aZTZKB4Aq2af1DbCIJNY3g3sh2X4Yg\",\n",
        "    user_agent=\"SatyaSrikarScript\"\n",
        ")\n",
        "\n",
        "def get_start_time(period):\n",
        "    now = datetime.utcnow()\n",
        "    if period == \"Day\":\n",
        "        return now - timedelta(days=1)\n",
        "    elif period == \"Week\":\n",
        "        return now - timedelta(weeks=1)\n",
        "    elif period == \"Month\":\n",
        "        return now - timedelta(days=30)\n",
        "    elif period == \"Year\":\n",
        "        return now - timedelta(days=365)\n",
        "    return now\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    score = sia.polarity_scores(text)['compound']\n",
        "    if score >= 0.05:\n",
        "        return 'Positive', score, 'green'\n",
        "    elif score <= -0.05:\n",
        "        return 'Negative', score, 'red'\n",
        "    return 'Neutral', score, 'gray'\n",
        "\n",
        "def img_html(fig):\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format=\"png\", bbox_inches='tight')\n",
        "    buf.seek(0)\n",
        "    encoded = base64.b64encode(buf.read()).decode()\n",
        "    buf.close()\n",
        "    plt.close()\n",
        "    return f'<img src=\"data:image/png;base64,{encoded}\"/>'\n",
        "\n",
        "def generate_wordcloud(posts):\n",
        "    text = \" \".join(p[\"title\"] for p in posts)\n",
        "    wc = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    return img_html(plt)\n",
        "\n",
        "def plot_sentiment_distribution(posts):\n",
        "    counts = Counter(p['sentiment_category'] for p in posts)\n",
        "    labels, values = zip(*counts.items()) if counts else ([], [])\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.pie(values, labels=labels, autopct='%1.1f%%', colors=['green', 'gray', 'red'])\n",
        "    return img_html(plt)\n",
        "\n",
        "def plot_top_words(posts):\n",
        "    words = \" \".join(p['title'] for p in posts).lower().split()\n",
        "    filtered = [w for w in words if w not in stop_words and len(w) > 1]\n",
        "    most_common = Counter(filtered).most_common(10)\n",
        "    if not most_common:\n",
        "        return \"<b>No keywords to display.</b>\"\n",
        "    labels, values = zip(*most_common)\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.bar(labels, values, color='skyblue')\n",
        "    return img_html(plt)\n",
        "\n",
        "def topic_modeling(posts):\n",
        "    texts = [p['title'] for p in posts]\n",
        "    if len(texts) < 3:\n",
        "        return \"<b>Insufficient data for topic modeling.</b>\"\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
        "    lda.fit(X)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    topics = []\n",
        "    for topic in lda.components_:\n",
        "        topic_words = [feature_names[i] for i in topic.argsort()[:-6:-1]]\n",
        "        topics.append(\", \".join(topic_words))\n",
        "    return \"<b>Topics:</b><br>\" + \"<br>\".join([f\"- {t}\" for t in topics])\n",
        "\n",
        "def extract_keywords(text):\n",
        "    doc = nlp(text)\n",
        "    keywords = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return \", \".join(keywords)\n",
        "\n",
        "def extract_named_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = defaultdict(int)\n",
        "    for ent in doc.ents:\n",
        "        entities[ent.text] += 1\n",
        "    return \"<br>\".join([f\"{ent}: {count}\" for ent, count in entities.items()])\n",
        "\n",
        "def calculate_influence_score(sentiment_score, engagement):\n",
        "    return sentiment_score + np.log(1 + engagement) * 0.1\n",
        "\n",
        "def fetch_reddit(keyword, period):\n",
        "    start_time = get_start_time(period)\n",
        "    posts = []\n",
        "    subreddits = ['all', 'worldnews', 'technology', 'sports', 'news', 'science', 'business', 'politics']\n",
        "    for subreddit in subreddits:\n",
        "        for submission in reddit.subreddit(subreddit).search(keyword, sort='new', limit=50):\n",
        "            post_time = datetime.utcfromtimestamp(submission.created_utc)\n",
        "            if post_time < start_time:\n",
        "                continue\n",
        "            text = submission.title + \" \" + (submission.selftext or \"\")\n",
        "            sent_cat, sent_score, sent_color = analyze_sentiment(text)\n",
        "            engagement = submission.score\n",
        "            influence_score = calculate_influence_score(sent_score, engagement)\n",
        "            posts.append({\n",
        "                'title': submission.title,\n",
        "                'source': 'Reddit',\n",
        "                'published': str(post_time),\n",
        "                'url': submission.url,\n",
        "                'sentiment_category': sent_cat,\n",
        "                'sentiment_score': sent_score,\n",
        "                'sentiment_color': sent_color,\n",
        "                'influence_score': influence_score\n",
        "            })\n",
        "    return posts\n",
        "\n",
        "def fetch_gnews(keyword, period):\n",
        "    start_time = get_start_time(period)\n",
        "    url = f\"https://gnews.io/api/v4/search?q={keyword}&lang=en&sortby=publishedAt&max=50&apikey={GNEWS_API_KEY}\"\n",
        "    res = requests.get(url).json()\n",
        "    posts = []\n",
        "    for a in res.get(\"articles\", []):\n",
        "        try:\n",
        "            pub_date = datetime.strptime(a['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "            if pub_date < start_time:\n",
        "                continue\n",
        "        except:\n",
        "            continue\n",
        "        text = a['title'] + \" \" + a.get(\"description\", \"\")\n",
        "        sent_cat, sent_score, sent_color = analyze_sentiment(text)\n",
        "        engagement = a.get(\"popularity\", 0)\n",
        "        influence_score = calculate_influence_score(sent_score, engagement)\n",
        "        posts.append({\n",
        "            'title': a['title'],\n",
        "            'source': a['source']['name'],\n",
        "            'published': a['publishedAt'],\n",
        "            'url': a['url'],\n",
        "            'sentiment_category': sent_cat,\n",
        "            'sentiment_score': sent_score,\n",
        "            'sentiment_color': sent_color,\n",
        "            'influence_score': influence_score\n",
        "        })\n",
        "    return posts\n",
        "\n",
        "def fetch_newsapi(keyword, period):\n",
        "    start_time = get_start_time(period)\n",
        "    url = f\"https://newsapi.org/v2/everything?q={keyword}&language=en&sortBy=publishedAt&pageSize=100&apiKey={NEWSAPI_KEY}\"\n",
        "    res = requests.get(url).json()\n",
        "    posts = []\n",
        "    for a in res.get(\"articles\", []):\n",
        "        try:\n",
        "            pub_date = datetime.strptime(a['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "            if pub_date < start_time:\n",
        "                continue\n",
        "        except:\n",
        "            continue\n",
        "        text = a['title'] + \" \" + a.get(\"description\", \"\")\n",
        "        sent_cat, sent_score, sent_color = analyze_sentiment(text)\n",
        "        engagement = a.get(\"popularity\", 0)\n",
        "        influence_score = calculate_influence_score(sent_score, engagement)\n",
        "        posts.append({\n",
        "            'title': a['title'],\n",
        "            'source': a['source']['name'],\n",
        "            'published': a['publishedAt'],\n",
        "            'url': a['url'],\n",
        "            'sentiment_category': sent_cat,\n",
        "            'sentiment_score': sent_score,\n",
        "            'sentiment_color': sent_color,\n",
        "            'influence_score': influence_score\n",
        "        })\n",
        "    return posts\n",
        "\n",
        "def fetch_rss(keyword, period):\n",
        "    start_time = get_start_time(period)\n",
        "    urls = [\n",
        "        \"https://rss.cnn.com/rss/cnn_topstories.rss\",\n",
        "        \"https://feeds.bbci.co.uk/news/rss.xml\",\n",
        "        \"https://techcrunch.com/feed/\"\n",
        "    ]\n",
        "    posts = []\n",
        "    for url in urls:\n",
        "        feed = feedparser.parse(url)\n",
        "        for entry in feed.entries:\n",
        "            try:\n",
        "                pub_date = datetime(*entry.published_parsed[:6])\n",
        "                if pub_date < start_time:\n",
        "                    continue\n",
        "            except:\n",
        "                continue\n",
        "            if keyword.lower() in entry.title.lower():\n",
        "                sent_cat, sent_score, sent_color = analyze_sentiment(entry.title)\n",
        "                influence_score = calculate_influence_score(sent_score, len(entry.title))\n",
        "                posts.append({\n",
        "                    'title': entry.title,\n",
        "                    'source': 'RSS Feed',\n",
        "                    'published': entry.published,\n",
        "                    'url': entry.link,\n",
        "                    'sentiment_category': sent_cat,\n",
        "                    'sentiment_score': sent_score,\n",
        "                    'sentiment_color': sent_color,\n",
        "                    'influence_score': influence_score\n",
        "                })\n",
        "    return posts\n",
        "\n",
        "def fetch_all(keyword, period):\n",
        "    posts = []\n",
        "    posts += fetch_reddit(keyword, period)\n",
        "    posts += fetch_gnews(keyword, period)\n",
        "    posts += fetch_newsapi(keyword, period)\n",
        "    posts += fetch_rss(keyword, period)\n",
        "    return posts\n",
        "\n",
        "def generate_classification_report(posts):\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    for p in posts:\n",
        "        true_labels.append(p['sentiment_category'])\n",
        "        predicted_labels.append(p['sentiment_category'])\n",
        "\n",
        "    report = classification_report(true_labels, predicted_labels, target_names=['Positive', 'Neutral', 'Negative'])\n",
        "\n",
        "    return report\n",
        "\n",
        "def format_output(posts):\n",
        "    if not posts:\n",
        "        return \"<b>No results found.</b>\"\n",
        "\n",
        "    html = \"<h3>🗳️ Top Influential Posts</h3>\"\n",
        "    posts = sorted(posts, key=lambda x: x['influence_score'], reverse=True)[:50]\n",
        "\n",
        "    for i, p in enumerate(posts, 1):\n",
        "        html += f\"\"\"\n",
        "        <div style='padding:10px;border-bottom:1px solid #ccc;'>\n",
        "            <b>#{i}</b> <a href='{p['url']}' target='_blank'>{p['title']}</a><br>\n",
        "            📅 {p['published']} | 📰 {p['source']}<br>\n",
        "            ❤️ Sentiment: <span style='color:{p['sentiment_color']};'>{p['sentiment_category']}</span>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "    html += \"<br><h4>☁️ Word Cloud</h4>\" + generate_wordcloud(posts)\n",
        "    html += \"<h4>📊 Sentiment Distribution</h4>\" + plot_sentiment_distribution(posts)\n",
        "    html += \"<h4>🔠 Frequent Words</h4>\" + plot_top_words(posts)\n",
        "    html += \"<h4>🧠 Topic Modeling</h4>\" + topic_modeling(posts)\n",
        "    html += \"<h4>🔑 Extracted Keywords</h4>\" + \", \".join([extract_keywords(p['title']) for p in posts])\n",
        "    html += \"<h4>🧬 Named Entities</h4>\" + \"<br>\".join([extract_named_entities(p['title']) for p in posts])\n",
        "\n",
        "    # Generate the classification report\n",
        "   # classification_rep = generate_classification_report(posts)\n",
        "   # html += f\"<h4>📊 Sentiment Classification Report</h4><pre>{classification_rep}</pre>\"\n",
        "\n",
        "    return html\n",
        "\n",
        "def main_func(keyword, period):\n",
        "    posts = fetch_all(keyword, period)\n",
        "    return format_output(posts)\n",
        "\n",
        "# Gradio UI\n",
        "interface = gr.Interface(\n",
        "    fn=main_func,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Keyword or Topic\"),\n",
        "        gr.Dropdown(choices=[\"Day\", \"Week\", \"Month\", \"Year\"], label=\"Time Period\")\n",
        "    ],\n",
        "    outputs=gr.HTML(label=\"Trending Insights\"),\n",
        "    title=\"🧠 Real-Time Detection of Trending Topics via Multi-Source Aggregation and Natural Language Processing\",\n",
        "\n",
        "    description=\"Aggregates content from Reddit, GNews, NewsAPI, and RSS. Provides sentiment analysis, influence scoring, word cloud, keywords, NER, and topic modeling.\"\n",
        ")\n",
        "\n",
        "interface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "pSWltZOSiaus",
        "outputId": "a4fe357d-9ffd-4b9f-cf4e-0750dab64fa2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8035072d21aacf5192.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8035072d21aacf5192.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}